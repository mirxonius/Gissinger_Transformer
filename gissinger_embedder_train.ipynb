{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Koopman embedding models for the Gissinger system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from trphysx.config.configuration_auto import AutoPhysConfig\n",
    "from trphysx.embedding.embedding_auto import AutoEmbeddingModel\n",
    "from trphysx.embedding.training import *\n",
    "from trphysx.embedding import embedding_gissinger as EG\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/gissinger/\"\n",
    "\n",
    "\n",
    "argv = []\n",
    "argv = argv + [\"--exp_name\", \"gissinger\"]\n",
    "argv = argv + [\n",
    "    \"--training_h5_file\",\n",
    "     data_dir + \"gissinger_training_rk.hdf5\"]\n",
    "argv = argv + [\n",
    "    \"--eval_h5_file\", \n",
    "    data_dir + \"gissinger_valid_rk.hdf5\"\n",
    "                ]\n",
    "argv = argv + [\n",
    "    \"--exp_dir\",\n",
    "    \"./outputs/embedding_gissinger\"\n",
    "     ]\n",
    "argv = argv + [\"--batch_size\", '512']\n",
    "argv = argv + [\"--block_size\", \"16\"]\n",
    "argv = argv + [\"--n_train\", \"2048\"]\n",
    "argv = argv + [\"--n_eval\", \"64\"]\n",
    "argv = argv + [\"--epochs\", \"200\"]\n",
    "\n",
    "\n",
    "args = EmbeddingParser().parse(args = argv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Physical configuration and getting handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoPhysConfig.load_config(args.exp_name)\n",
    "dataloader = AutoDataHandler.load_data_handler(args.exp_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training and testing dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mirksonius/Desktop/seminar_fizika/kod/trphysx/embedding/training/enn_data_handler.py:239: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  data_series = torch.Tensor(f[key])\n"
     ]
    }
   ],
   "source": [
    "training_loader = dataloader.createTrainingLoader(\n",
    "    args.training_h5_file, \n",
    "    block_size=args.block_size, \n",
    "    stride=args.stride, \n",
    "    ndata=args.n_train, \n",
    "    batch_size=args.batch_size)\n",
    "testing_loader = dataloader.createTestingLoader(\n",
    "    args.eval_h5_file, \n",
    "    block_size=32, \n",
    "    ndata=args.n_eval, \n",
    "    batch_size=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting devices for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch device:cuda:0\n"
     ]
    }
   ],
   "source": [
    "if(torch.cuda.is_available()):\n",
    "    use_cuda = \"cuda\"\n",
    "args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Torch device:{}\".format(args.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing and loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEmbeddingModel.init_trainer(args.exp_name, config).to(args.device)\n",
    "mu, std = dataloader.norm_params\n",
    "model.embedding_model.mu = mu.to(args.device)\n",
    "model.embedding_model.std = std.to(args.device)\n",
    "if args.epoch_start > 1:\n",
    "  model.load_model(args.ckpt_dir, args.epoch_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining opttimizers and learning scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr*0.995**(args.epoch_start-1), weight_decay=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validating model\n",
    "Validation is perfomed every 5 epochs on a separate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "124it [00:11, 10.98it/s]\n",
      "01/16/2023 18:20:57 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 1: Training loss 571466.062, Lr 0.00100\n",
      "01/16/2023 18:20:57 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 1: Test loss: 0.00\n",
      "124it [00:08, 14.50it/s]\n",
      "01/16/2023 18:21:05 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 2: Training loss 20755.508, Lr 0.00099\n",
      "124it [00:09, 12.69it/s]\n",
      "01/16/2023 18:21:15 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 3: Training loss 18191.412, Lr 0.00099\n",
      "124it [00:09, 13.02it/s]\n",
      "01/16/2023 18:21:25 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 4: Training loss 12170.416, Lr 0.00099\n",
      "124it [00:09, 13.54it/s]\n",
      "01/16/2023 18:21:34 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 5: Training loss 10104.886, Lr 0.00098\n",
      "01/16/2023 18:21:34 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 5: Test loss: 0.00\n",
      "124it [00:09, 13.00it/s]\n",
      "01/16/2023 18:21:43 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 6: Training loss 16105.922, Lr 0.00098\n",
      "124it [00:07, 16.06it/s]\n",
      "01/16/2023 18:21:51 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 7: Training loss 10639.752, Lr 0.00097\n",
      "124it [00:09, 13.13it/s]\n",
      "01/16/2023 18:22:01 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 8: Training loss 7517.475, Lr 0.00097\n",
      "124it [00:09, 12.74it/s]\n",
      "01/16/2023 18:22:10 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 9: Training loss 9314.400, Lr 0.00096\n",
      "124it [00:09, 13.31it/s]\n",
      "01/16/2023 18:22:20 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 10: Training loss 7481.572, Lr 0.00096\n",
      "01/16/2023 18:22:20 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 10: Test loss: 0.00\n",
      "124it [00:09, 13.57it/s]\n",
      "01/16/2023 18:22:29 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 11: Training loss 6616.710, Lr 0.00095\n",
      "124it [00:09, 13.09it/s]\n",
      "01/16/2023 18:22:38 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 12: Training loss 7401.992, Lr 0.00095\n",
      "124it [00:09, 13.06it/s]\n",
      "01/16/2023 18:22:48 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 13: Training loss 7853.365, Lr 0.00094\n",
      "124it [00:08, 14.23it/s]\n",
      "01/16/2023 18:22:56 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 14: Training loss 6047.565, Lr 0.00094\n",
      "124it [00:07, 15.69it/s]\n",
      "01/16/2023 18:23:04 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 15: Training loss 5442.490, Lr 0.00093\n",
      "01/16/2023 18:23:04 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 15: Test loss: 0.00\n",
      "124it [00:11, 10.90it/s]\n",
      "01/16/2023 18:23:16 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 16: Training loss 4923.959, Lr 0.00093\n",
      "124it [00:11, 11.17it/s]\n",
      "01/16/2023 18:23:27 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 17: Training loss 4426.873, Lr 0.00092\n",
      "124it [00:09, 12.62it/s]\n",
      "01/16/2023 18:23:37 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 18: Training loss 4206.426, Lr 0.00092\n",
      "124it [00:09, 13.30it/s]\n",
      "01/16/2023 18:23:46 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 19: Training loss 4749.109, Lr 0.00091\n",
      "124it [00:09, 12.53it/s]\n",
      "01/16/2023 18:23:56 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 20: Training loss 6646.875, Lr 0.00091\n",
      "01/16/2023 18:23:56 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 20: Test loss: 0.00\n",
      "124it [00:08, 14.33it/s]\n",
      "01/16/2023 18:24:05 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 21: Training loss 3630.674, Lr 0.00090\n",
      "124it [00:07, 15.83it/s]\n",
      "01/16/2023 18:24:12 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 22: Training loss 3613.586, Lr 0.00090\n",
      "124it [00:08, 13.92it/s]\n",
      "01/16/2023 18:24:21 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 23: Training loss 6964.621, Lr 0.00090\n",
      "124it [00:09, 13.26it/s]\n",
      "01/16/2023 18:24:31 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 24: Training loss 5195.244, Lr 0.00089\n",
      "124it [00:10, 11.93it/s]\n",
      "01/16/2023 18:24:41 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 25: Training loss 5777.609, Lr 0.00089\n",
      "01/16/2023 18:24:41 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 25: Test loss: 0.00\n",
      "01/16/2023 18:24:41 - INFO - trphysx.embedding.training.enn_trainer -   Checkpointing model, optimizer and scheduler.\n",
      "124it [00:08, 15.24it/s]\n",
      "01/16/2023 18:24:49 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 26: Training loss 4100.043, Lr 0.00088\n",
      "124it [00:09, 13.37it/s]\n",
      "01/16/2023 18:24:59 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 27: Training loss 3411.000, Lr 0.00088\n",
      "124it [00:09, 13.42it/s]\n",
      "01/16/2023 18:25:08 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 28: Training loss 3297.275, Lr 0.00087\n",
      "124it [00:08, 14.28it/s]\n",
      "01/16/2023 18:25:16 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 29: Training loss 3010.380, Lr 0.00087\n",
      "124it [00:08, 14.47it/s]\n",
      "01/16/2023 18:25:25 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 30: Training loss 3868.809, Lr 0.00086\n",
      "01/16/2023 18:25:25 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 30: Test loss: 0.00\n",
      "124it [00:08, 14.11it/s]\n",
      "01/16/2023 18:25:34 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 31: Training loss 3108.632, Lr 0.00086\n",
      "124it [00:07, 15.69it/s]\n",
      "01/16/2023 18:25:42 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 32: Training loss 2878.725, Lr 0.00086\n",
      "124it [00:08, 13.90it/s]\n",
      "01/16/2023 18:25:51 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 33: Training loss 2783.216, Lr 0.00085\n",
      "124it [00:10, 12.18it/s]\n",
      "01/16/2023 18:26:01 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 34: Training loss 5101.760, Lr 0.00085\n",
      "124it [00:08, 13.87it/s]\n",
      "01/16/2023 18:26:10 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 35: Training loss 3538.318, Lr 0.00084\n",
      "01/16/2023 18:26:10 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 35: Test loss: 0.00\n",
      "124it [00:08, 15.10it/s]\n",
      "01/16/2023 18:26:18 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 36: Training loss 3365.246, Lr 0.00084\n",
      "124it [00:08, 13.80it/s]\n",
      "01/16/2023 18:26:27 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 37: Training loss 2607.251, Lr 0.00083\n",
      "124it [00:08, 14.79it/s]\n",
      "01/16/2023 18:26:35 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 38: Training loss 2893.988, Lr 0.00083\n",
      "124it [00:09, 13.39it/s]\n",
      "01/16/2023 18:26:45 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 39: Training loss 2426.591, Lr 0.00083\n",
      "124it [00:08, 13.90it/s]\n",
      "01/16/2023 18:26:54 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 40: Training loss 2303.588, Lr 0.00082\n",
      "01/16/2023 18:26:54 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 40: Test loss: 0.00\n",
      "124it [00:09, 12.94it/s]\n",
      "01/16/2023 18:27:03 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 41: Training loss 2196.582, Lr 0.00082\n",
      "124it [00:09, 12.64it/s]\n",
      "01/16/2023 18:27:13 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 42: Training loss 2134.923, Lr 0.00081\n",
      "124it [00:08, 14.37it/s]\n",
      "01/16/2023 18:27:22 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 43: Training loss 2197.469, Lr 0.00081\n",
      "124it [00:09, 13.52it/s]\n",
      "01/16/2023 18:27:31 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 44: Training loss 2929.820, Lr 0.00081\n",
      "124it [00:08, 13.86it/s]\n",
      "01/16/2023 18:27:40 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 45: Training loss 2048.035, Lr 0.00080\n",
      "01/16/2023 18:27:40 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 45: Test loss: 0.00\n",
      "124it [00:08, 14.27it/s]\n",
      "01/16/2023 18:27:49 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 46: Training loss 1992.731, Lr 0.00080\n",
      "124it [00:08, 13.98it/s]\n",
      "01/16/2023 18:27:57 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 47: Training loss 2125.788, Lr 0.00079\n",
      "124it [00:08, 14.41it/s]\n",
      "01/16/2023 18:28:06 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 48: Training loss 5188.378, Lr 0.00079\n",
      "124it [00:08, 14.08it/s]\n",
      "01/16/2023 18:28:15 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 49: Training loss 2775.168, Lr 0.00079\n",
      "124it [00:08, 13.79it/s]\n",
      "01/16/2023 18:28:24 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 50: Training loss 4785.787, Lr 0.00078\n",
      "01/16/2023 18:28:24 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 50: Test loss: 0.00\n",
      "01/16/2023 18:28:24 - INFO - trphysx.embedding.training.enn_trainer -   Checkpointing model, optimizer and scheduler.\n",
      "124it [00:08, 14.10it/s]\n",
      "01/16/2023 18:28:33 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 51: Training loss 3730.321, Lr 0.00078\n",
      "124it [00:08, 14.43it/s]\n",
      "01/16/2023 18:28:41 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 52: Training loss 2020.009, Lr 0.00077\n",
      "124it [00:07, 16.60it/s]\n",
      "01/16/2023 18:28:49 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 53: Training loss 1880.665, Lr 0.00077\n",
      "124it [00:08, 14.34it/s]\n",
      "01/16/2023 18:28:57 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 54: Training loss 1830.611, Lr 0.00077\n",
      "124it [00:08, 15.26it/s]\n",
      "01/16/2023 18:29:06 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 55: Training loss 2424.851, Lr 0.00076\n",
      "01/16/2023 18:29:06 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 55: Test loss: 0.00\n",
      "124it [00:08, 15.50it/s]\n",
      "01/16/2023 18:29:14 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 56: Training loss 1882.390, Lr 0.00076\n",
      "124it [00:09, 12.81it/s]\n",
      "01/16/2023 18:29:23 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 57: Training loss 1828.458, Lr 0.00076\n",
      "124it [00:08, 14.27it/s]\n",
      "01/16/2023 18:29:32 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 58: Training loss 1730.438, Lr 0.00075\n",
      "124it [00:09, 13.56it/s]\n",
      "01/16/2023 18:29:41 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 59: Training loss 1787.001, Lr 0.00075\n",
      "124it [00:10, 12.09it/s]\n",
      "01/16/2023 18:29:51 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 60: Training loss 2558.732, Lr 0.00074\n",
      "01/16/2023 18:29:51 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 60: Test loss: 0.00\n",
      "124it [00:09, 12.91it/s]\n",
      "01/16/2023 18:30:01 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 61: Training loss 2000.787, Lr 0.00074\n",
      "124it [00:07, 15.73it/s]\n",
      "01/16/2023 18:30:09 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 62: Training loss 1718.584, Lr 0.00074\n",
      "124it [00:09, 13.60it/s]\n",
      "01/16/2023 18:30:18 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 63: Training loss 1889.286, Lr 0.00073\n",
      "124it [00:07, 16.42it/s]\n",
      "01/16/2023 18:30:26 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 64: Training loss 2638.212, Lr 0.00073\n",
      "124it [00:07, 15.81it/s]\n",
      "01/16/2023 18:30:33 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 65: Training loss 1829.497, Lr 0.00073\n",
      "01/16/2023 18:30:33 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 65: Test loss: 0.00\n",
      "124it [00:07, 17.18it/s]\n",
      "01/16/2023 18:30:41 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 66: Training loss 1671.015, Lr 0.00072\n",
      "124it [00:08, 14.38it/s]\n",
      "01/16/2023 18:30:49 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 67: Training loss 1676.285, Lr 0.00072\n",
      "124it [00:07, 15.83it/s]\n",
      "01/16/2023 18:30:57 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 68: Training loss 2199.142, Lr 0.00071\n",
      "124it [00:08, 15.42it/s]\n",
      "01/16/2023 18:31:05 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 69: Training loss 1925.121, Lr 0.00071\n",
      "124it [00:09, 13.52it/s]\n",
      "01/16/2023 18:31:14 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 70: Training loss 1596.598, Lr 0.00071\n",
      "01/16/2023 18:31:14 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 70: Test loss: 0.00\n",
      "124it [00:08, 14.79it/s]\n",
      "01/16/2023 18:31:23 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 71: Training loss 1523.150, Lr 0.00070\n",
      "124it [00:08, 15.45it/s]\n",
      "01/16/2023 18:31:31 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 72: Training loss 1794.685, Lr 0.00070\n",
      "124it [00:08, 14.77it/s]\n",
      "01/16/2023 18:31:39 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 73: Training loss 1513.442, Lr 0.00070\n",
      "124it [00:08, 14.45it/s]\n",
      "01/16/2023 18:31:48 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 74: Training loss 1491.528, Lr 0.00069\n",
      "124it [00:08, 13.95it/s]\n",
      "01/16/2023 18:31:57 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 75: Training loss 1457.187, Lr 0.00069\n",
      "01/16/2023 18:31:57 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 75: Test loss: 0.00\n",
      "01/16/2023 18:31:57 - INFO - trphysx.embedding.training.enn_trainer -   Checkpointing model, optimizer and scheduler.\n",
      "124it [00:08, 14.67it/s]\n",
      "01/16/2023 18:32:05 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 76: Training loss 2138.694, Lr 0.00069\n",
      "124it [00:07, 15.57it/s]\n",
      "01/16/2023 18:32:13 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 77: Training loss 1935.526, Lr 0.00068\n",
      "124it [00:07, 15.89it/s]\n",
      "01/16/2023 18:32:21 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 78: Training loss 1431.162, Lr 0.00068\n",
      "124it [00:07, 16.11it/s]\n",
      "01/16/2023 18:32:29 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 79: Training loss 1411.848, Lr 0.00068\n",
      "124it [00:08, 15.30it/s]\n",
      "01/16/2023 18:32:37 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 80: Training loss 2059.756, Lr 0.00067\n",
      "01/16/2023 18:32:37 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 80: Test loss: 0.00\n",
      "124it [00:09, 13.45it/s]\n",
      "01/16/2023 18:32:46 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 81: Training loss 1511.415, Lr 0.00067\n",
      "124it [00:07, 15.65it/s]\n",
      "01/16/2023 18:32:54 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 82: Training loss 1378.825, Lr 0.00067\n",
      "124it [00:06, 17.88it/s]\n",
      "01/16/2023 18:33:01 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 83: Training loss 2337.641, Lr 0.00066\n",
      "124it [00:07, 16.77it/s]\n",
      "01/16/2023 18:33:08 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 84: Training loss 1349.512, Lr 0.00066\n",
      "124it [00:07, 15.70it/s]\n",
      "01/16/2023 18:33:16 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 85: Training loss 1312.042, Lr 0.00066\n",
      "01/16/2023 18:33:16 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 85: Test loss: 0.00\n",
      "124it [00:08, 14.92it/s]\n",
      "01/16/2023 18:33:24 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 86: Training loss 1318.949, Lr 0.00065\n",
      "124it [00:07, 15.62it/s]\n",
      "01/16/2023 18:33:32 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 87: Training loss 1271.282, Lr 0.00065\n",
      "124it [00:08, 15.49it/s]\n",
      "01/16/2023 18:33:40 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 88: Training loss 1243.252, Lr 0.00065\n",
      "124it [00:08, 15.33it/s]\n",
      "01/16/2023 18:33:48 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 89: Training loss 1275.102, Lr 0.00064\n",
      "124it [00:07, 15.72it/s]\n",
      "01/16/2023 18:33:56 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 90: Training loss 1680.870, Lr 0.00064\n",
      "01/16/2023 18:33:56 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 90: Test loss: 0.00\n",
      "124it [00:08, 15.03it/s]\n",
      "01/16/2023 18:34:05 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 91: Training loss 1491.875, Lr 0.00064\n",
      "124it [00:07, 16.84it/s]\n",
      "01/16/2023 18:34:12 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 92: Training loss 1362.941, Lr 0.00063\n",
      "124it [00:07, 16.01it/s]\n",
      "01/16/2023 18:34:20 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 93: Training loss 1711.221, Lr 0.00063\n",
      "124it [00:08, 15.49it/s]\n",
      "01/16/2023 18:34:28 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 94: Training loss 1351.220, Lr 0.00063\n",
      "124it [00:07, 16.72it/s]\n",
      "01/16/2023 18:34:35 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 95: Training loss 1205.631, Lr 0.00062\n",
      "01/16/2023 18:34:35 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 95: Test loss: 0.00\n",
      "124it [00:08, 15.39it/s]\n",
      "01/16/2023 18:34:43 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 96: Training loss 1199.484, Lr 0.00062\n",
      "124it [00:07, 16.73it/s]\n",
      "01/16/2023 18:34:51 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 97: Training loss 1232.828, Lr 0.00062\n",
      "124it [00:07, 16.49it/s]\n",
      "01/16/2023 18:34:58 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 98: Training loss 1483.780, Lr 0.00061\n",
      "124it [00:07, 15.78it/s]\n",
      "01/16/2023 18:35:06 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 99: Training loss 1394.921, Lr 0.00061\n",
      "124it [00:07, 15.97it/s]\n",
      "01/16/2023 18:35:14 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 100: Training loss 2113.598, Lr 0.00061\n",
      "01/16/2023 18:35:14 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 100: Test loss: 0.00\n",
      "01/16/2023 18:35:14 - INFO - trphysx.embedding.training.enn_trainer -   Checkpointing model, optimizer and scheduler.\n",
      "124it [00:07, 16.48it/s]\n",
      "01/16/2023 18:35:21 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 101: Training loss 1855.598, Lr 0.00061\n",
      "124it [00:08, 15.44it/s]\n",
      "01/16/2023 18:35:29 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 102: Training loss 1257.607, Lr 0.00060\n",
      "124it [00:07, 16.63it/s]\n",
      "01/16/2023 18:35:37 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 103: Training loss 1323.049, Lr 0.00060\n",
      "124it [00:07, 16.29it/s]\n",
      "01/16/2023 18:35:44 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 104: Training loss 1186.737, Lr 0.00060\n",
      "124it [00:07, 16.20it/s]\n",
      "01/16/2023 18:35:52 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 105: Training loss 1093.927, Lr 0.00059\n",
      "01/16/2023 18:35:52 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 105: Test loss: 0.00\n",
      "124it [00:07, 16.13it/s]\n",
      "01/16/2023 18:36:00 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 106: Training loss 1074.464, Lr 0.00059\n",
      "124it [00:07, 16.44it/s]\n",
      "01/16/2023 18:36:07 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 107: Training loss 1076.835, Lr 0.00059\n",
      "124it [00:07, 16.70it/s]\n",
      "01/16/2023 18:36:15 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 108: Training loss 1049.819, Lr 0.00058\n",
      "124it [00:07, 16.18it/s]\n",
      "01/16/2023 18:36:22 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 109: Training loss 1050.989, Lr 0.00058\n",
      "124it [00:07, 16.03it/s]\n",
      "01/16/2023 18:36:30 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 110: Training loss 1233.265, Lr 0.00058\n",
      "01/16/2023 18:36:30 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 110: Test loss: 0.00\n",
      "124it [00:07, 16.91it/s]\n",
      "01/16/2023 18:36:38 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 111: Training loss 1085.848, Lr 0.00058\n",
      "124it [00:07, 15.66it/s]\n",
      "01/16/2023 18:36:46 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 112: Training loss 1048.349, Lr 0.00057\n",
      "124it [00:07, 15.60it/s]\n",
      "01/16/2023 18:36:53 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 113: Training loss 1171.960, Lr 0.00057\n",
      "124it [00:07, 17.14it/s]\n",
      "01/16/2023 18:37:01 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 114: Training loss 1398.343, Lr 0.00057\n",
      "124it [00:07, 17.44it/s]\n",
      "01/16/2023 18:37:08 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 115: Training loss 1406.961, Lr 0.00056\n",
      "01/16/2023 18:37:08 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 115: Test loss: 0.00\n",
      "124it [00:07, 16.67it/s]\n",
      "01/16/2023 18:37:15 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 116: Training loss 1305.067, Lr 0.00056\n",
      "124it [00:07, 15.87it/s]\n",
      "01/16/2023 18:37:23 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 117: Training loss 1126.835, Lr 0.00056\n",
      "124it [00:08, 15.09it/s]\n",
      "01/16/2023 18:37:31 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 118: Training loss 1210.771, Lr 0.00056\n",
      "124it [00:07, 15.90it/s]\n",
      "01/16/2023 18:37:39 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 119: Training loss 1119.383, Lr 0.00055\n",
      "124it [00:07, 15.76it/s]\n",
      "01/16/2023 18:37:47 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 120: Training loss 1139.086, Lr 0.00055\n",
      "01/16/2023 18:37:47 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 120: Test loss: 0.00\n",
      "124it [00:07, 15.75it/s]\n",
      "01/16/2023 18:37:55 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 121: Training loss 1206.325, Lr 0.00055\n",
      "124it [00:09, 13.77it/s]\n",
      "01/16/2023 18:38:04 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 122: Training loss 1040.260, Lr 0.00055\n",
      "124it [00:07, 16.62it/s]\n",
      "01/16/2023 18:38:11 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 123: Training loss 990.994, Lr 0.00054\n",
      "124it [00:07, 15.69it/s]\n",
      "01/16/2023 18:38:19 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 124: Training loss 967.087, Lr 0.00054\n",
      "124it [00:07, 15.90it/s]\n",
      "01/16/2023 18:38:27 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 125: Training loss 960.140, Lr 0.00054\n",
      "01/16/2023 18:38:27 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 125: Test loss: 0.00\n",
      "01/16/2023 18:38:27 - INFO - trphysx.embedding.training.enn_trainer -   Checkpointing model, optimizer and scheduler.\n",
      "124it [00:08, 15.32it/s]\n",
      "01/16/2023 18:38:35 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 126: Training loss 955.633, Lr 0.00053\n",
      "124it [00:08, 15.35it/s]\n",
      "01/16/2023 18:38:43 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 127: Training loss 1082.232, Lr 0.00053\n",
      "124it [00:08, 14.42it/s]\n",
      "01/16/2023 18:38:52 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 128: Training loss 1115.801, Lr 0.00053\n",
      "124it [00:08, 15.42it/s]\n",
      "01/16/2023 18:39:00 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 129: Training loss 1073.043, Lr 0.00053\n",
      "124it [00:08, 14.47it/s]\n",
      "01/16/2023 18:39:08 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 130: Training loss 1161.289, Lr 0.00052\n",
      "01/16/2023 18:39:09 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 130: Test loss: 0.00\n",
      "124it [00:07, 16.66it/s]\n",
      "01/16/2023 18:39:16 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 131: Training loss 1131.434, Lr 0.00052\n",
      "124it [00:07, 16.07it/s]\n",
      "01/16/2023 18:39:24 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 132: Training loss 1167.016, Lr 0.00052\n",
      "124it [00:08, 14.77it/s]\n",
      "01/16/2023 18:39:32 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 133: Training loss 942.605, Lr 0.00052\n",
      "124it [00:07, 16.34it/s]\n",
      "01/16/2023 18:39:40 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 134: Training loss 915.815, Lr 0.00051\n",
      "124it [00:07, 16.68it/s]\n",
      "01/16/2023 18:39:47 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 135: Training loss 915.018, Lr 0.00051\n",
      "01/16/2023 18:39:47 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 135: Test loss: 0.00\n",
      "124it [00:08, 15.38it/s]\n",
      "01/16/2023 18:39:55 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 136: Training loss 909.612, Lr 0.00051\n",
      "124it [00:07, 17.44it/s]\n",
      "01/16/2023 18:40:02 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 137: Training loss 889.482, Lr 0.00051\n",
      "124it [00:08, 14.83it/s]\n",
      "01/16/2023 18:40:11 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 138: Training loss 902.044, Lr 0.00050\n",
      "124it [00:08, 15.14it/s]\n",
      "01/16/2023 18:40:19 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 139: Training loss 938.151, Lr 0.00050\n",
      "124it [00:07, 16.26it/s]\n",
      "01/16/2023 18:40:26 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 140: Training loss 1165.896, Lr 0.00050\n",
      "01/16/2023 18:40:27 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 140: Test loss: 0.00\n",
      "124it [00:07, 16.81it/s]\n",
      "01/16/2023 18:40:34 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 141: Training loss 1037.997, Lr 0.00050\n",
      "124it [00:07, 16.82it/s]\n",
      "01/16/2023 18:40:41 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 142: Training loss 1149.145, Lr 0.00049\n",
      "124it [00:07, 16.39it/s]\n",
      "01/16/2023 18:40:49 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 143: Training loss 1310.124, Lr 0.00049\n",
      "124it [00:07, 15.67it/s]\n",
      "01/16/2023 18:40:57 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 144: Training loss 1065.107, Lr 0.00049\n",
      "124it [00:07, 16.85it/s]\n",
      "01/16/2023 18:41:04 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 145: Training loss 959.108, Lr 0.00049\n",
      "01/16/2023 18:41:04 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 145: Test loss: 0.00\n",
      "124it [00:07, 16.11it/s]\n",
      "01/16/2023 18:41:12 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 146: Training loss 928.195, Lr 0.00048\n",
      "124it [00:07, 16.01it/s]\n",
      "01/16/2023 18:41:20 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 147: Training loss 963.522, Lr 0.00048\n",
      "124it [00:07, 16.25it/s]\n",
      "01/16/2023 18:41:27 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 148: Training loss 879.118, Lr 0.00048\n",
      "124it [00:07, 16.98it/s]\n",
      "01/16/2023 18:41:35 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 149: Training loss 868.318, Lr 0.00048\n",
      "124it [00:07, 16.56it/s]\n",
      "01/16/2023 18:41:42 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 150: Training loss 865.615, Lr 0.00047\n",
      "01/16/2023 18:41:42 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 150: Test loss: 0.00\n",
      "01/16/2023 18:41:42 - INFO - trphysx.embedding.training.enn_trainer -   Checkpointing model, optimizer and scheduler.\n",
      "124it [00:07, 15.96it/s]\n",
      "01/16/2023 18:41:50 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 151: Training loss 856.594, Lr 0.00047\n",
      "124it [00:08, 14.93it/s]\n",
      "01/16/2023 18:41:58 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 152: Training loss 860.748, Lr 0.00047\n",
      "124it [00:08, 15.37it/s]\n",
      "01/16/2023 18:42:06 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 153: Training loss 902.402, Lr 0.00047\n",
      "124it [00:07, 16.35it/s]\n",
      "01/16/2023 18:42:14 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 154: Training loss 1020.095, Lr 0.00046\n",
      "124it [00:07, 17.23it/s]\n",
      "01/16/2023 18:42:21 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 155: Training loss 1016.612, Lr 0.00046\n",
      "01/16/2023 18:42:21 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 155: Test loss: 0.00\n",
      "124it [00:07, 16.15it/s]\n",
      "01/16/2023 18:42:29 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 156: Training loss 925.090, Lr 0.00046\n",
      "124it [00:07, 15.99it/s]\n",
      "01/16/2023 18:42:36 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 157: Training loss 858.193, Lr 0.00046\n",
      "124it [00:07, 15.65it/s]\n",
      "01/16/2023 18:42:44 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 158: Training loss 849.446, Lr 0.00046\n",
      "124it [00:08, 14.81it/s]\n",
      "01/16/2023 18:42:53 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 159: Training loss 858.046, Lr 0.00045\n",
      "124it [00:07, 15.79it/s]\n",
      "01/16/2023 18:43:01 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 160: Training loss 889.692, Lr 0.00045\n",
      "01/16/2023 18:43:01 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 160: Test loss: 0.00\n",
      "124it [00:07, 17.60it/s]\n",
      "01/16/2023 18:43:08 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 161: Training loss 908.438, Lr 0.00045\n",
      "124it [00:07, 17.16it/s]\n",
      "01/16/2023 18:43:15 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 162: Training loss 875.403, Lr 0.00045\n",
      "124it [00:08, 15.05it/s]\n",
      "01/16/2023 18:43:23 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 163: Training loss 849.184, Lr 0.00044\n",
      "124it [00:07, 16.34it/s]\n",
      "01/16/2023 18:43:31 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 164: Training loss 839.540, Lr 0.00044\n",
      "124it [00:08, 15.10it/s]\n",
      "01/16/2023 18:43:39 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 165: Training loss 1040.585, Lr 0.00044\n",
      "01/16/2023 18:43:39 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 165: Test loss: 0.00\n",
      "124it [00:07, 17.37it/s]\n",
      "01/16/2023 18:43:46 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 166: Training loss 906.697, Lr 0.00044\n",
      "124it [00:08, 14.60it/s]\n",
      "01/16/2023 18:43:55 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 167: Training loss 817.875, Lr 0.00044\n",
      "124it [00:08, 14.47it/s]\n",
      "01/16/2023 18:44:03 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 168: Training loss 903.361, Lr 0.00043\n",
      "124it [00:07, 16.67it/s]\n",
      "01/16/2023 18:44:11 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 169: Training loss 929.080, Lr 0.00043\n",
      "124it [00:08, 14.89it/s]\n",
      "01/16/2023 18:44:19 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 170: Training loss 829.626, Lr 0.00043\n",
      "01/16/2023 18:44:19 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 170: Test loss: 0.00\n",
      "124it [00:07, 16.16it/s]\n",
      "01/16/2023 18:44:27 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 171: Training loss 834.916, Lr 0.00043\n",
      "124it [00:08, 15.29it/s]\n",
      "01/16/2023 18:44:35 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 172: Training loss 833.032, Lr 0.00042\n",
      "124it [00:07, 16.31it/s]\n",
      "01/16/2023 18:44:42 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 173: Training loss 829.121, Lr 0.00042\n",
      "124it [00:07, 16.28it/s]\n",
      "01/16/2023 18:44:50 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 174: Training loss 945.140, Lr 0.00042\n",
      "124it [00:07, 15.69it/s]\n",
      "01/16/2023 18:44:58 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 175: Training loss 842.172, Lr 0.00042\n",
      "01/16/2023 18:44:58 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 175: Test loss: 0.00\n",
      "01/16/2023 18:44:58 - INFO - trphysx.embedding.training.enn_trainer -   Checkpointing model, optimizer and scheduler.\n",
      "124it [00:07, 17.21it/s]\n",
      "01/16/2023 18:45:05 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 176: Training loss 794.952, Lr 0.00042\n",
      "124it [00:07, 15.69it/s]\n",
      "01/16/2023 18:45:13 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 177: Training loss 790.123, Lr 0.00041\n",
      "124it [00:08, 14.31it/s]\n",
      "01/16/2023 18:45:22 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 178: Training loss 783.741, Lr 0.00041\n",
      "124it [00:08, 14.48it/s]\n",
      "01/16/2023 18:45:30 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 179: Training loss 787.074, Lr 0.00041\n",
      "124it [00:08, 15.22it/s]\n",
      "01/16/2023 18:45:38 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 180: Training loss 838.041, Lr 0.00041\n",
      "01/16/2023 18:45:38 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 180: Test loss: 0.00\n",
      "124it [00:07, 15.98it/s]\n",
      "01/16/2023 18:45:46 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 181: Training loss 903.608, Lr 0.00041\n",
      "124it [00:07, 16.23it/s]\n",
      "01/16/2023 18:45:54 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 182: Training loss 1094.199, Lr 0.00040\n",
      "124it [00:08, 13.97it/s]\n",
      "01/16/2023 18:46:03 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 183: Training loss 911.201, Lr 0.00040\n",
      "124it [00:08, 14.16it/s]\n",
      "01/16/2023 18:46:11 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 184: Training loss 779.047, Lr 0.00040\n",
      "124it [00:09, 12.74it/s]\n",
      "01/16/2023 18:46:21 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 185: Training loss 764.545, Lr 0.00040\n",
      "01/16/2023 18:46:21 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 185: Test loss: 0.00\n",
      "124it [00:08, 15.49it/s]\n",
      "01/16/2023 18:46:29 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 186: Training loss 853.284, Lr 0.00040\n",
      "124it [00:07, 16.27it/s]\n",
      "01/16/2023 18:46:37 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 187: Training loss 1005.938, Lr 0.00039\n",
      "124it [00:08, 14.01it/s]\n",
      "01/16/2023 18:46:46 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 188: Training loss 794.119, Lr 0.00039\n",
      "124it [00:08, 15.11it/s]\n",
      "01/16/2023 18:46:54 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 189: Training loss 807.895, Lr 0.00039\n",
      "124it [00:08, 14.36it/s]\n",
      "01/16/2023 18:47:03 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 190: Training loss 933.526, Lr 0.00039\n",
      "01/16/2023 18:47:03 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 190: Test loss: 0.00\n",
      "124it [00:07, 15.67it/s]\n",
      "01/16/2023 18:47:11 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 191: Training loss 771.912, Lr 0.00039\n",
      "124it [00:08, 15.22it/s]\n",
      "01/16/2023 18:47:19 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 192: Training loss 739.159, Lr 0.00038\n",
      "124it [00:08, 14.55it/s]\n",
      "01/16/2023 18:47:27 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 193: Training loss 736.558, Lr 0.00038\n",
      "124it [00:07, 16.55it/s]\n",
      "01/16/2023 18:47:35 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 194: Training loss 738.949, Lr 0.00038\n",
      "124it [00:08, 14.77it/s]\n",
      "01/16/2023 18:47:43 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 195: Training loss 728.674, Lr 0.00038\n",
      "01/16/2023 18:47:43 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 195: Test loss: 0.00\n",
      "124it [00:08, 14.81it/s]\n",
      "01/16/2023 18:47:52 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 196: Training loss 722.323, Lr 0.00038\n",
      "124it [00:07, 15.95it/s]\n",
      "01/16/2023 18:47:59 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 197: Training loss 717.772, Lr 0.00037\n",
      "124it [00:08, 14.44it/s]\n",
      "01/16/2023 18:48:08 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 198: Training loss 724.613, Lr 0.00037\n",
      "124it [00:08, 14.54it/s]\n",
      "01/16/2023 18:48:16 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 199: Training loss 736.870, Lr 0.00037\n",
      "124it [00:07, 16.36it/s]\n",
      "01/16/2023 18:48:24 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 200: Training loss 915.282, Lr 0.00037\n",
      "01/16/2023 18:48:24 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 200: Test loss: 0.00\n",
      "01/16/2023 18:48:24 - INFO - trphysx.embedding.training.enn_trainer -   Checkpointing model, optimizer and scheduler.\n"
     ]
    }
   ],
   "source": [
    "trainer = EmbeddingTrainer(model, args, (optimizer, scheduler))\n",
    "trainer.train(training_loader, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('transphysx')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0039daf51d82cbdcdb3464804c7bcf8a6c3f809a41c74c3343bb30e42a6dc1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
